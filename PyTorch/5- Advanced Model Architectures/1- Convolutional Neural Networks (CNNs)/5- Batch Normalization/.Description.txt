Batch normalization normalizes the input to each layer in a neural network, improving training speed and stability.