Since Transformers do not have a built-in sense of sequence order, position encodings are added to the input embeddings to provide information about the position of tokens in the sequence.