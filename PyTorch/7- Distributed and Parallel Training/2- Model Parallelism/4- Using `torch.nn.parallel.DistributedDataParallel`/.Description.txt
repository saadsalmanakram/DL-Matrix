For large-scale distributed training across multiple nodes and GPUs, combining model parallelism with data parallelism.
