1- What is Reinforcement Learning?

Ans- Reinforcement Learning is a framework where an agent learns to solve tasks by interacting with an environment, receiving rewards as feedback for its actions.

(-------------------------------------------------------------------------)

2- How does an agent learn in Reinforcement Learning?

Ans- An agent learns through trial and error by taking actions in an environment and receiving rewards, either positive or negative, based on the outcomes.

(-------------------------------------------------------------------------)

3- Can you provide a real-world analogy for Reinforcement Learning?

Ans- It’s like a child learning to play a video game: by trial and error, they figure out which actions lead to success and which to failure, improving over time.

(-------------------------------------------------------------------------)

4- What does the environment represent in Reinforcement Learning?

Ans- The environment is the external system with which the agent interacts, providing feedback based on the agent’s actions.

(-------------------------------------------------------------------------)

5- What is the role of rewards in Reinforcement Learning?

Ans- Rewards guide the agent by indicating which actions are beneficial (+1) and which are detrimental (-1) to achieving the task.

(-------------------------------------------------------------------------)

6- How is learning achieved without supervision in Reinforcement Learning?

Ans- Learning is achieved through self-discovery, where the agent independently explores actions and their outcomes without any external guidance.

(-------------------------------------------------------------------------)

7- What is the ultimate goal of an agent in Reinforcement Learning?

Ans- The ultimate goal is to maximize cumulative rewards over time by choosing the best actions.

(-------------------------------------------------------------------------)

8- How is trial and error utilized in Reinforcement Learning?

Ans- The agent experiments with different actions, learns from the consequences, and improves its strategy to achieve better results.

(-------------------------------------------------------------------------)

9- Why is Reinforcement Learning considered a computational approach?

Ans- It uses algorithms and computations to mimic the natural learning process of humans and animals through interaction with the environment.

(-------------------------------------------------------------------------)

10- What kind of tasks can Reinforcement Learning solve?

Ans- Reinforcement Learning is used to solve control tasks or decision problems, where an optimal series of actions needs to be determined.

(-------------------------------------------------------------------------)

11- What is the central idea of the Reinforcement Learning framework?

Ans- The central idea is the reward hypothesis, which states that all goals can be described as the maximization of expected cumulative reward.

(-------------------------------------------------------------------------)

12- What is the goal of an RL agent?

Ans- The goal is to maximize its cumulative reward, also known as the expected return.

(-------------------------------------------------------------------------)

13- Describe the RL process in simple terms.

Ans- The RL process involves an agent interacting with an environment by taking actions that lead to new states and receiving rewards, forming a loop.

(-------------------------------------------------------------------------)

14- What is the role of the environment in the RL process?

Ans- The environment provides the current state to the agent and responds to the agent's actions with a new state and a reward.

(-------------------------------------------------------------------------)

15- What does the sequence of state, action, reward, and next state represent in RL?

Ans- It represents the interaction loop where the agent takes actions based on the current state to maximize cumulative rewards.

(-------------------------------------------------------------------------)

16- What is a state in the context of RL?

Ans- A state is a representation of the current situation or condition of the environment from which the agent makes decisions.

(-------------------------------------------------------------------------)

17- What is an action in RL?

Ans- An action is a decision or move made by the agent to interact with the environment.

(-------------------------------------------------------------------------)

18- What is the reward in RL?

Ans- A reward is the feedback from the environment, indicating how good or bad the agent's action was.

(-------------------------------------------------------------------------)

19- What does "next state" refer to in RL?

Ans- The next state is the new condition of the environment after the agent has taken an action.

(-------------------------------------------------------------------------)

20- What is the Markov Property in RL?

Ans- The Markov Property implies that the agent's decision depends only on the current state, not on the sequence of previous states and actions.

(-------------------------------------------------------------------------)

21- Why is the Markov Property important in RL?

Ans- It simplifies the decision-making process by ensuring that only the current state is needed to make optimal decisions.

(-------------------------------------------------------------------------)

22- What is the difference between an observation and a state in RL?

Ans- A state is a complete description of the environment, while an observation is a partial view of the state, common in partially observed environments.

(------------------------------------------------------------------------)

23- What is an action space in RL?

Ans- The action space is the set of all possible actions the agent can take in the environment.

(------------------------------------------------------------------------)

24- What is the difference between discrete and continuous action spaces?

Ans- Discrete action spaces have a finite set of actions, while continuous action spaces have an infinite number of possible actions.

(------------------------------------------------------------------------)

25- Why are rewards fundamental in RL?

Ans- Rewards are the only feedback the agent receives to learn whether an action was beneficial or not.

(------------------------------------------------------------------------)

26- What is a discount rate in RL?

Ans- The discount rate (gamma) determines how much future rewards are valued compared to immediate rewards.

(------------------------------------------------------------------------)

27- How does a high discount rate (gamma) affect an agent's behavior?

Ans- A high gamma means the agent values long-term rewards more, leading to more future-oriented actions.

(------------------------------------------------------------------------)

28- How does a low discount rate (gamma) affect an agent's behavior?

Ans- A low gamma means the agent values short-term rewards more, leading to actions focused on immediate gains.

(------------------------------------------------------------------------)

29- What is a task in Reinforcement Learning?

Ans- A task is an instance of a Reinforcement Learning problem where an agent interacts with an environment to achieve a goal.

(------------------------------------------------------------------------)

30- What are the two types of tasks in Reinforcement Learning?

Ans- The two types of tasks are episodic tasks and continuing tasks.

(------------------------------------------------------------------------)

31- What defines an episodic task in Reinforcement Learning?

Ans- An episodic task has a clear starting point and an ending point (terminal state), forming an episode.

(------------------------------------------------------------------------)

32- Can you give an example of an episodic task?

Ans- An example of an episodic task is a Super Mario Bros level, where the episode starts at the beginning of a level and ends when Mario is either killed or completes the level.

(------------------------------------------------------------------------)

33- What is a continuing task in Reinforcement Learning?

Ans- A continuing task is a task with no terminal state, meaning it continues indefinitely until stopped.

(------------------------------------------------------------------------)

34- Can you provide an example of a continuing task?

Ans- An example of a continuing task is automated stock trading, where the agent continuously interacts with the market without a predefined endpoint.

(------------------------------------------------------------------------)

35- How does the learning approach differ between episodic and continuing tasks?

Ans- In episodic tasks, the agent learns based on episodes, while in continuing tasks, the agent continuously learns without an episode boundary.

(------------------------------------------------------------------------)

36- Why is it important to differentiate between episodic and continuing tasks?

Ans- Differentiating helps in designing appropriate learning algorithms and reward structures based on the nature of the task.

(------------------------------------------------------------------------)

37- What is an episode in the context of Reinforcement Learning?

Ans- An episode is a sequence of states, actions, rewards, and new states from the start to the terminal state in an episodic task.

(------------------------------------------------------------------------)

38- How does the concept of a terminal state relate to episodic tasks?

Ans- A terminal state marks the end of an episode in an episodic task.

(------------------------------------------------------------------------)

39- In which type of task is there no terminal state?

Ans- There is no terminal state in a continuing task.

(------------------------------------------------------------------------)

40- What does an agent need to focus on in continuing tasks?

Ans- The agent must focus on continuously optimizing its actions while interacting with the environment indefinitely.

(------------------------------------------------------------------------)

41- How might rewards be structured differently in episodic vs. continuing tasks?

Ans- In episodic tasks, rewards are accumulated over episodes, while in continuing tasks, rewards are often discounted over time to reflect ongoing performance.

(------------------------------------------------------------------------)

42- What is the key challenge in continuing tasks?

Ans- The key challenge is to maintain performance over an indefinite period without a clear end.

(------------------------------------------------------------------------)

43- How does the agent's goal differ between episodic and continuing tasks?

Ans- In episodic tasks, the goal is to maximize rewards within each episode, while in continuing tasks, the goal is to maximize long-term rewards over an indefinite timeline.

(------------------------------------------------------------------------)

44- What is the exploration/exploitation trade-off in Reinforcement Learning?

Ans- It refers to the dilemma of choosing between exploring new actions to gather more information (exploration) or using known actions to maximize rewards (exploitation).

(------------------------------------------------------------------------)

45- Why is the exploration/exploitation trade-off important in Reinforcement Learning?

Ans- Balancing exploration and exploitation is crucial for maximizing the expected cumulative reward of the RL agent.

(------------------------------------------------------------------------)

46- What happens if an RL agent focuses solely on exploitation?

Ans- The agent may miss out on potentially larger rewards because it only focuses on known sources of smaller rewards.

(------------------------------------------------------------------------)

47- What is the risk of focusing solely on exploration in RL?

Ans- The agent might spend too much time exploring without ever fully exploiting the known sources of rewards, leading to suboptimal performance.

(------------------------------------------------------------------------)

48- How does the example of a mouse in a maze illustrate the exploration/exploitation trade-off?

Ans- The mouse might continuously gather small rewards (exploitation) but miss out on a much larger reward unless it explores the maze (exploration).

(------------------------------------------------------------------------)

49- How can the choice of picking a restaurant serve as a real-world analogy for the exploration/exploitation trade-off?

Ans- You can either stick to a restaurant you know is good (exploitation) or try a new one with unknown quality (exploration), balancing the risk and potential reward.

(------------------------------------------------------------------------)

50- Why do RL agents need a rule to manage the exploration/exploitation trade-off?

Ans- A rule is necessary to balance exploration and exploitation effectively to ensure the agent maximizes its long-term reward.

(------------------------------------------------------------------------)

51- What could be a potential downside of excessive exploration?

Ans- Excessive exploration could lead to missed opportunities to maximize known rewards, resulting in lower overall performance.

(------------------------------------------------------------------------)

52- In the context of RL, what is the goal of balancing exploration and exploitation?

Ans- The goal is to maximize the expected cumulative reward over time by strategically exploring new possibilities and exploiting known rewards.

(------------------------------------------------------------------------)

53- What is an example of exploitation in the restaurant analogy?

Ans- Consistently going to the same restaurant that you know provides a good experience.

(------------------------------------------------------------------------)

54- What is an example of exploration in the restaurant analogy?

Ans- Trying out a new restaurant that could either be a great find or a disappointment.

(------------------------------------------------------------------------)

55- How can an RL agent fall into a trap without exploration?

Ans- The agent might get stuck exploiting a suboptimal strategy, never discovering a better one due to a lack of exploration.

(------------------------------------------------------------------------)

56- What does it mean to maximize the expected cumulative reward in RL?

Ans- It means making decisions that will lead to the highest total reward over time, considering both current and future actions.

(------------------------------------------------------------------------)

57- How can the exploration/exploitation trade-off affect long-term outcomes in RL?

Ans- The balance between exploration and exploitation determines whether the agent discovers optimal strategies or gets stuck with suboptimal ones.

(------------------------------------------------------------------------)

58- What are the two main approaches for solving RL problems?

Ans- The two main approaches are Policy-Based Methods and Value-Based Methods.

(------------------------------------------------------------------------)

59- How do we solve the RL problem?

Ans- We solve the RL problem by training an RL agent to select actions that maximize its expected cumulative reward.

(-------------------------------------------------------------------------)

60- What is the policy π in Reinforcement Learning?

Ans- The policy π is the agent's brain, determining the action to take given a specific state.

(-------------------------------------------------------------------------)

61- Why is the policy considered the agent’s brain?

Ans- The policy is the function that dictates the agent's behavior at any given time, guiding action selection.

(-------------------------------------------------------------------------)

62- What is the goal of learning a policy in RL?

Ans- The goal is to find the optimal policy π* that maximizes the expected return.

(-------------------------------------------------------------------------)

63- How do Policy-Based Methods work in RL?

Ans- Policy-Based Methods involve learning a policy function directly, mapping states to the best corresponding actions or probability distributions over actions.

(-------------------------------------------------------------------------)

64- What are the two types of policies in RL?

Ans- The two types are Deterministic policies and Stochastic policies.

(-------------------------------------------------------------------------)

65- What is a Deterministic policy in RL?

Ans- A Deterministic policy always returns the same action for a given state.

(-------------------------------------------------------------------------)

66- What is a Stochastic policy in RL?

Ans- A Stochastic policy outputs a probability distribution over possible actions given the current state.

(-------------------------------------------------------------------------)

67- How do Value-Based Methods work in RL?

Ans- Value-Based Methods involve learning a value function that maps states to their expected value, guiding actions to states with the highest value.

(-------------------------------------------------------------------------)

68- What does the value function represent in RL?

Ans- The value function represents the expected discounted return from being in a specific state.

(-------------------------------------------------------------------------)

69- How does a value function influence policy in Value-Based Methods?

Ans- The policy selects actions leading to the state with the highest value as defined by the value function.

(-------------------------------------------------------------------------)

70- What does it mean to "act according to our policy" in Value-Based Methods?

Ans- It means selecting actions that lead to states with the highest values based on the value function.

(-------------------------------------------------------------------------)

71- What is Deep Reinforcement Learning?

Ans- Deep Reinforcement Learning (Deep RL) is the application of deep neural networks to solve Reinforcement Learning problems.

(-------------------------------------------------------------------------)

72- How does Deep Reinforcement Learning differ from classic Reinforcement Learning?

Ans- Classic RL uses algorithms to create Q tables for decision-making, while Deep RL uses neural networks to approximate Q values.

(-------------------------------------------------------------------------)

73- What role does a neural network play in Deep Q-Learning?

Ans- In Deep Q-Learning, a neural network approximates the Q values, determining the best action for each state.

(-------------------------------------------------------------------------)

74- Why is it called "Deep" Reinforcement Learning?

Ans- It's called "Deep" because it incorporates deep neural networks into the traditional RL framework.

(-------------------------------------------------------------------------)

75- What is Q-Learning?

Ans- Q-Learning is a value-based RL algorithm that creates a Q table to map state-action pairs to expected rewards.

(-------------------------------------------------------------------------)

76- What is Deep Q-Learning?

Ans- Deep Q-Learning is an extension of Q-Learning that uses a neural network instead of a Q table to estimate action values.

(-------------------------------------------------------------------------)

77- Why might one choose Deep Q-Learning over traditional Q-Learning?

Ans- Deep Q-Learning is preferred when dealing with large or continuous state spaces where Q tables become impractical.

(-------------------------------------------------------------------------)

78- What is a Q table in Q-Learning?

Ans- A Q table is a lookup table that stores the expected rewards (Q values) for each state-action pair in Q-Learning.

(-------------------------------------------------------------------------)

79- What problem does Deep Reinforcement Learning solve that traditional methods struggle with?

Ans- Deep RL effectively handles high-dimensional state spaces, which are difficult for traditional RL methods.

(-------------------------------------------------------------------------)

80- What are value-based algorithms in Reinforcement Learning?

Ans- Value-based algorithms, like Q-Learning, focus on estimating the value of actions to make decisions.

(-------------------------------------------------------------------------)

81- How does the neural network in Deep Q-Learning improve over a Q table?

Ans- The neural network can generalize across similar states, handling larger and more complex state spaces than a Q table.

(-------------------------------------------------------------------------)

82- What kind of problems are best suited for Deep Reinforcement Learning?

Ans- Problems with large, complex, or continuous state spaces are best suited for Deep RL.

(-------------------------------------------------------------------------)

83- Can Deep Q-Learning be applied to real-world scenarios?

Ans- Yes, Deep Q-Learning is widely used in real-world applications like robotics, gaming, and autonomous systems.

(-------------------------------------------------------------------------)

84- What is the significance of the "value" in value-based RL algorithms?

Ans- The "value" refers to the expected cumulative reward from taking a specific action in a given state.

(-------------------------------------------------------------------------)

85- Why is familiarity with Deep Learning important for understanding Deep Reinforcement Learning?

Ans- Deep Learning knowledge is crucial for understanding how neural networks are used to approximate functions in Deep RL.

(-------------------------------------------------------------------------)

86- What is Huggy in the context of Deep Reinforcement Learning?

Ans- Huggy is a Deep Reinforcement Learning environment by Hugging Face, based on Puppo the Corgi, created using Unity and MLAgents.

(-------------------------------------------------------------------------)

87- Which game engine is used to create the Huggy environment?

Ans- The Huggy environment is created using the Unity game engine.

(-------------------------------------------------------------------------)

88- What is ML-Agents?

Ans- ML-Agents is a toolkit by Unity that allows the creation and training of agents in environments built with Unity.

(-------------------------------------------------------------------------)

89- What is a value function in reinforcement learning?

Ans- A value function maps a state to the expected return (discounted future rewards) an agent can obtain starting from that state.

(-------------------------------------------------------------------------)

90- What is the difference between value-based and policy-based methods?

Ans- Value-based methods train a value function to guide actions, while policy-based methods directly train the policy to select actions.

(-------------------------------------------------------------------------)

91- What is the goal of a reinforcement learning agent?

Ans- The goal is to learn an optimal policy that maximizes the expected return.

(-------------------------------------------------------------------------)

92- How does a policy function in value-based methods?

Ans- In value-based methods, the policy is typically predefined, such as a greedy policy, and is not directly trained.

(-------------------------------------------------------------------------)

93- What is the role of a Greedy Policy in value-based methods?

Ans- A Greedy Policy selects the action that maximizes the value function at each state.

(-------------------------------------------------------------------------)

94- How do value-based methods relate to the optimal policy?

Ans- The optimal policy can be derived from an optimal value function, often by selecting actions that maximize the value.

(-------------------------------------------------------------------------)

95- What is the state-value function?

Ans- The state-value function gives the expected return starting from a specific state and following a policy.

(-------------------------------------------------------------------------)

96- What is the action-value function?

Ans- The action-value function provides the expected return for taking a specific action in a specific state and then following a policy.

(-------------------------------------------------------------------------)

97- How does the action-value function differ from the state-value function?

Ans- The state-value function evaluates the value of a state, while the action-value function evaluates the value of a state-action pair.

(-------------------------------------------------------------------------)

98- Why is the Bellman equation important in value-based methods?

Ans- The Bellman equation helps recursively compute the value function efficiently by breaking down the problem into smaller subproblems.

(-------------------------------------------------------------------------)

99- What does the Bellman equation calculate?

Ans- It calculates the expected return for a state or state-action pair based on the immediate reward and the value of the next state.

(-------------------------------------------------------------------------)

100- What is the Epsilon-Greedy Policy?

Ans- The Epsilon-Greedy Policy balances exploration and exploitation by choosing a random action with probability ε and the best-known action with probability 1-ε.

(-------------------------------------------------------------------------)

101- Why is exploration important in reinforcement learning?

Ans- Exploration helps the agent discover new actions that may lead to higher rewards, preventing it from getting stuck in suboptimal policies.

(-------------------------------------------------------------------------)

102- What challenge does the Bellman equation address in value-based methods?

Ans- It addresses the computational complexity of calculating the value of all possible state or state-action pairs by providing a recursive approach.

(-------------------------------------------------------------------------)

103- How does an RL agent learn from its environment?

Ans- By interacting with the environment and using the experience and rewards to update its value function or policy.

(------------------------------------------------------------------------)

104- What is the key difference between Monte Carlo and Temporal Difference Learning?

Ans- Monte Carlo uses an entire episode of experience, while Temporal Difference uses a single step to learn.

(------------------------------------------------------------------------)

105- When does Monte Carlo update the value function?

Ans- At the end of an episode.

(------------------------------------------------------------------------)

106- What is required for Monte Carlo to update the value function?

Ans- A complete episode of interaction.

(------------------------------------------------------------------------)

107- What type of strategy does the agent use in Monte Carlo learning?

Ans- An epsilon-greedy strategy, balancing exploration and exploitation.

(------------------------------------------------------------------------)

108- When does Temporal Difference (TD) update the value function?

Ans- After each step of interaction with the environment.

(------------------------------------------------------------------------)

109- What is the TD target in Temporal Difference learning?

Ans- The sum of the immediate reward and the discounted value of the next state.

(------------------------------------------------------------------------)

110- What is bootstrapping in Temporal Difference learning?

Ans- Using an existing estimate rather than waiting for the full return.

(------------------------------------------------------------------------)

111- What is TD(0) or one-step TD?

Ans- A method that updates the value function after any individual step.

(------------------------------------------------------------------------)

112- What is the primary advantage of Temporal Difference over Monte Carlo?

Ans- It updates the value function more frequently, after each step, rather than waiting for a complete episode.

(------------------------------------------------------------------------)

113- What is Q-Learning?

Ans- Q-Learning is an off-policy, value-based reinforcement learning algorithm that uses temporal difference (TD) methods to learn the optimal action-value function.

(------------------------------------------------------------------------)

114- What does 'off-policy' mean in Q-Learning?

Ans- Off-policy means that Q-Learning learns the value of the optimal policy independently of the agent's actions, allowing it to learn from an exploratory policy but optimize the greedy one.

(------------------------------------------------------------------------)

115- What is the primary goal of Q-Learning?

Ans- The primary goal of Q-Learning is to find the optimal policy by learning the action-value function (Q-function) that maximizes the cumulative reward.

(------------------------------------------------------------------------)

116- What is a Q-function?

Ans- The Q-function is an action-value function that outputs the expected cumulative reward of taking a specific action in a given state.

(------------------------------------------------------------------------)

117- What is the difference between value and reward in Q-Learning?

Ans- The value is the expected cumulative reward starting from a state, while the reward is the immediate feedback received after taking an action in that state.

(------------------------------------------------------------------------)

118- What is a Q-table?

Ans- A Q-table is a data structure that stores the Q-values for each state-action pair, serving as the memory of the Q-function.

(------------------------------------------------------------------------)

119- How is the Q-table initialized in Q-Learning?

Ans- The Q-table is typically initialized with arbitrary values, often zeros, before training begins.

(------------------------------------------------------------------------)

120- What is the epsilon-greedy strategy in Q-Learning?

Ans- The epsilon-greedy strategy balances exploration and exploitation by choosing a random action with probability ɛ and the best-known action with probability 1-ɛ.

(------------------------------------------------------------------------)

121- Why is the epsilon-greedy strategy used in Q-Learning?

Ans- It is used to ensure the agent explores the environment sufficiently before converging on the optimal policy.

(------------------------------------------------------------------------)

122- How is the Q-value updated in Q-Learning?

Ans- The Q-value is updated using the Bellman equation, which incorporates the immediate reward and the maximum Q-value of the next state-action pair.

(------------------------------------------------------------------------)

123- What does it mean to 'bootstrap' in Q-Learning?

Ans- Bootstrapping in Q-Learning refers to updating the Q-value based on the current estimate of future rewards, rather than waiting for the final outcome.

(------------------------------------------------------------------------)

124- How does Q-Learning handle the exploration-exploitation trade-off?

Ans- Q-Learning handles this trade-off through the epsilon-greedy strategy, gradually reducing exploration as the Q-table improves.

(------------------------------------------------------------------------)

125- What is the main difference between off-policy and on-policy algorithms?

Ans- Off-policy algorithms, like Q-Learning, use different policies for acting and updating, whereas on-policy algorithms, like Sarsa, use the same policy for both.

(------------------------------------------------------------------------)

126- How does the Q-Learning algorithm converge to the optimal policy?

Ans- The Q-Learning algorithm converges to the optimal policy by iteratively updating the Q-values based on the actions taken and rewards received, eventually leading to the optimal Q-table.

(------------------------------------------------------------------------)

127- What is the role of the discount factor in Q-Learning?

Ans- The discount factor in Q-Learning determines the importance of future rewards compared to immediate rewards, guiding the learning process toward long-term success.

(------------------------------------------------------------------------)

128- Why is Q-Learning considered an off-policy algorithm?

Ans- Q-Learning is off-policy because it updates the Q-value using the best possible action (greedy policy) for the next state, regardless of the action actually taken by the agent (epsilon-greedy policy).

(------------------------------------------------------------------------)

129- What happens when the Q-table is fully trained?

Ans- When the Q-table is fully trained, it contains the optimal Q-values for each state-action pair, allowing the agent to follow the optimal policy.

(------------------------------------------------------------------------)

130- Can Q-Learning be applied to continuous state spaces?

Ans- Q-Learning is generally used for discrete state spaces, but variations like Deep Q-Learning can handle continuous state spaces by approximating the Q-function with neural networks.

(------------------------------------------------------------------------)

131- Why is Deep Q-Learning preferred over traditional Q-Learning in large state spaces?

Ans- Deep Q-Learning is preferred because it can handle large state spaces by approximating the Q-values with a neural network, whereas traditional Q-Learning becomes impractical due to the exponential growth of the Q-table.

(------------------------------------------------------------------------)

132- What is the role of the neural network in Deep Q-Learning?

Ans- The neural network in Deep Q-Learning approximates the Q-value function, which estimates the expected rewards for each action given a state.

(------------------------------------------------------------------------)

133- How does Deep Q-Learning handle exploration vs. exploitation?

Ans- Deep Q-Learning typically uses an ε-greedy policy, where ε controls the trade-off between exploration (choosing random actions) and exploitation (choosing the action with the highest predicted Q-value).

(------------------------------------------------------------------------)

134- What is the purpose of the experience replay in Deep Q-Learning?

Ans- Experience replay improves learning efficiency and stability by storing past experiences and randomly sampling them to break the correlation between consecutive learning steps.

(------------------------------------------------------------------------)

